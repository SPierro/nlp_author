{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os \n",
    "import re\n",
    "import spacy \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "col = [\"File_Name\",\"Auteur\", \"Texte\"]\n",
    "df= pd.DataFrame(columns=col)\n",
    "print(df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "chemin= os.getcwd() + '\\\\training_data\\\\'\n",
    "chemin=str(chemin)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "datas=[]\n",
    "file_dir=[]\n",
    "file_name=[]\n",
    "\n",
    "for file in os.listdir(chemin):\n",
    "    file_name.append(file)\n",
    "    file_dir= chemin+str(file)\n",
    "    with open(file_dir, 'r') as f:\n",
    "        data = f.read()\n",
    "        datas.append(data)\n",
    "        f.close()\n",
    "print(datas)\n",
    "print(file_name)\n",
    "df=pd.DataFrame({\"Texte\":[], \"File_Name\":[]})\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df[\"Texte\"]=datas\n",
    "df[\"File_Name\"]=file_name\n",
    "print(df)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pattern= \"[A-Z]{3}[.]+txt$\"\n",
    "search = []    \n",
    "for values in df[\"File_Name\"]:\n",
    "    search.append(re.search(r'[A-Z]{3}', values).group())\n",
    "df['Auteur'] = search\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Modèle entrainé le plus léger et suffisant ici. Regarder si md ou lrg à meilleur ratio time_process/accurancy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Code plus efficient afin de tokeniser, lemmatiser et rajouter les POS. \n",
    "# Ne pas oublier de rajouter les stopwords!!!!!\n",
    "tokens = []\n",
    "lemma = []\n",
    "pos = []\n",
    "stop_words= []\n",
    "ent= []\n",
    "\n",
    "for doc in nlp.pipe(df['Texte'].astype('unicode').values, batch_size=50, n_threads=3):\n",
    "    if doc.is_parsed:\n",
    "        tokens.append([n.text for n in doc])\n",
    "        lemma.append([n.lemma_ for n in doc])\n",
    "        pos.append([n.pos_ for n in doc])\n",
    "        stop_words.append([n.text for n in doc if not n.is_stop])\n",
    "        ent.append([e.label_ for e in doc.ents])\n",
    "\n",
    "    else:\n",
    "        # Ajouter des blancs si erreur pour avoir le même nombre d'entrées\n",
    "        lemma.append(None)\n",
    "        pos.append(None)\n",
    "        stop_words.append(None)\n",
    "        ent.append(None)\n",
    "\n",
    "df['Tokens'] = tokens\n",
    "df['Tokens_NoStopW']= stop_words\n",
    "df['lemma'] = lemma\n",
    "df['PartOfSpeech'] = pos\n",
    "df['ent'] = ent"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "DEBUT DE L'EXPLORATION"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "auteur = df.groupby(\"Auteur\")\n",
    "auteur.describe().head()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Comptage des occurences des mots :"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df['clean_text']=str()\n",
    "for i, row in df.iterrows():\n",
    "    row['clean_text']=' '.join(row['lemma'])\n",
    "    \n",
    "\n",
    "df['clean_text'] = df.clean_text.replace(\"[PRON\\s\\W]\", \" \",regex=True)\n",
    "df['clean_text'] = df.clean_text.replace(' +',' ',regex=True)\n",
    "df['clean_text'] = df.clean_text.replace('^ ','',regex=True)\n",
    "\n",
    "#df = pd.get_dummies(df, columns=['Auteur'])\n",
    "df['Auteur_number']=df['Auteur'].map({'EAP': 1, 'HPL':2, 'MWS':3})\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_finale=df[['Auteur_number','clean_text','ent']]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y = df['Auteur_number']\n",
    "#X=df_finale.drop(['Auteur_number'], axis=1)\n",
    "X= df['Texte']\n",
    "print(X)\n",
    "print(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "auteurs=['EAP','HPL', 'MWS']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#On va transformer les mots en vecteurs sur base de tfidf\n",
    "tfv = TfidfVectorizer(min_df=3, analyzer='word',ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1)\n",
    "tfv.fit(list(X_train) + list(X_test))\n",
    "X_train_tfv =  tfv.transform(X_train) \n",
    "X_test_tfv = tfv.transform(X_test)\n",
    "\n",
    "#On fait la même chose mais sur base de comptage des mots\n",
    "ctv = CountVectorizer(analyzer='word',ngram_range=(1, 3))\n",
    "ctv.fit(list(X_train) + list(X_test))\n",
    "X_train_ctv =  ctv.transform(X_train) \n",
    "X_test_ctv = ctv.transform(X_test)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Test avec une Multinomial regression\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_ctv, y_train)\n",
    "y_pred_NB = nb.predict(X_test_ctv)\n",
    "print('accuracy %s' % accuracy_score(y_pred_NB, y_test))\n",
    "print(classification_report(y_test, y_pred_NB,target_names=auteurs))\n",
    "\n",
    "scores = cross_val_score(nb, X_test_ctv, y_test, cv=10)\n",
    "print(\"Cross Val Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Test avec une LinearSVC\n",
    "svc = LinearSVC()\n",
    "svc.fit(X_train_ctv, y_train)\n",
    "y_pred_svc = svc.predict(X_test_ctv)\n",
    "print('accuracy %s' % accuracy_score(y_pred_svc, y_test))\n",
    "print(classification_report(y_test, y_pred_svc,target_names=auteurs))\n",
    "\n",
    "scores = cross_val_score(svc, X_test_ctv, y_test, cv=10)\n",
    "print(\"Cross Val Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Test avec GradientBoost\n",
    "gb = GradientBoostingClassifier(max_depth=6, n_estimators=10, random_state=2)\n",
    "gb.fit(X_train_ctv,y_train)\n",
    "y_pred_boost = gb.predict(X_test_ctv)\n",
    "print('accuracy %s' % accuracy_score(y_pred_boost, y_test))\n",
    "print(classification_report(y_test, y_pred_boost,target_names=auteurs))\n",
    "\n",
    "scores = cross_val_score(gb, X_test_ctv, y_test, cv=10)\n",
    "print(\"Cross Val Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Test avec KNN\n",
    "kn = KNeighborsClassifier(n_neighbors=1)\n",
    "kn.fit(X_train_ctv,y_train)\n",
    "y_pred_kn = kn.predict(X_test_ctv)\n",
    "print('accuracy %s' % accuracy_score(y_pred_kn, y_test))\n",
    "print(classification_report(y_test, y_pred_kn,target_names=auteurs))\n",
    "\n",
    "scores = cross_val_score(kn, X_test_ctv, y_test, cv=10)\n",
    "print(\"Cross Val Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Test avec SVM\n",
    "sv = SGDClassifier()\n",
    "sv.fit(X_train_ctv,y_train)\n",
    "y_pred_svm = sv.predict(X_test_ctv)\n",
    "print('accuracy %s' % accuracy_score(y_pred_svm, y_test))\n",
    "print(classification_report(y_test, y_pred_svm,target_names=auteurs))\n",
    "\n",
    "scores = cross_val_score(sv, X_test_ctv, y_test, cv=10)\n",
    "print(\"Cross Val Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Test avec Beroulli\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "bnb = BernoulliNB()\n",
    "bnb.fit(X_train_ctv,y_train)\n",
    "y_pred_bnb = bnb.predict(X_test_ctv)\n",
    "print('accuracy %s' % accuracy_score(y_pred_bnb, y_test))\n",
    "print(classification_report(y_test, y_pred_bnb,target_names=auteurs))\n",
    "\n",
    "scores = cross_val_score(bnb, X_test_ctv, y_test, cv=10)\n",
    "print(\"Cross Val Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import CategoricalNB\n",
    "cnb = SGDClassifier()\n",
    "cnb.fit(X_train_ctv,y_train)\n",
    "y_pred_cnb = cnb.predict(X_test_ctv)\n",
    "print('accuracy %s' % accuracy_score(y_pred_cnb, y_test))\n",
    "print(classification_report(y_test, y_pred_cnb,target_names=auteurs))\n",
    "\n",
    "scores = cross_val_score(cnb, X_test_ctv, y_test, cv=10)\n",
    "print(\"Cross Val Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Test avec RandomForest\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=1000, min_samples_leaf=0.0001)\n",
    "rf.fit(X_train_ctv,y_train)\n",
    "y_pred_rfo = rf.predict(X_test_ctv)\n",
    "print('accuracy %s' % accuracy_score(y_pred_rfo, y_test))\n",
    "print(classification_report(y_test, y_pred_rfo,target_names=auteurs))\n",
    "\n",
    "scores = cross_val_score(rf, X_test_ctv, y_test, cv=10)\n",
    "print(\"Cross Val Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Test avec Logistic regression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train_ctv,y_train)\n",
    "y_pred_lrg = lr.predict(X_test_ctv)\n",
    "print('accuracy %s' % accuracy_score(y_pred_lrg, y_test))\n",
    "print(classification_report(y_test, y_pred_lrg,target_names=auteurs))\n",
    "\n",
    "scores = cross_val_score(lr, X_test_ctv, y_test, cv=10)\n",
    "print(\"Cross Val Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "clf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n",
    "                        subsample=0.8, nthread=10, learning_rate=0.1)\n",
    "clf.fit(X_train_ctv.tocsc(), y_train)\n",
    "y_pred = clf.predict(X_test_ctv)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "\n",
    "scores = cross_val_score(clf, X_test_ctv, y_test, cv=10)\n",
    "print(\"Cross Val Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}