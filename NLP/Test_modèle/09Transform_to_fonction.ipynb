{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import spacy\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud\n",
    "import nltk\n",
    "\n",
    "'''Variables qui seront créées:'''\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "col = [\"Auteur\",'Auteur_number', \"Texte\", \"File_Name\",'Tokens','Tokens_NoStopW','lemma','PartOfSpeech','ent']\n",
    "df=pd.DataFrame(columns=col)\n",
    "chemin= os.getcwd() + '\\\\training_data\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def to_Dataframe(chemin):\n",
    "    '''Cette fonction va ajouter les fichiers txt au dataframe df et va retourner la df '''\n",
    "    datas = []\n",
    "    file_dir = []\n",
    "    file_name = []\n",
    "    chemin=str(chemin)\n",
    "    for file in os.listdir(chemin):\n",
    "        file_name.append(file)\n",
    "        file_dir= chemin+str(file)\n",
    "        with open(file_dir, 'r') as f:\n",
    "            data = f.read()\n",
    "            datas.append(data)\n",
    "            f.close()\n",
    "    df[\"Texte\"]=datas\n",
    "    df[\"File_Name\"]=file_name\n",
    "    pattern= \"[A-Z]{3}[.]+txt$\"\n",
    "    search = []\n",
    "    for values in df[\"File_Name\"]:\n",
    "        search.append(re.search(r'[A-Z]{3}', values).group())\n",
    "    df['Auteur'] = search\n",
    "    #df = pd.get_dummies(df, columns=['Auteur'])\n",
    "    df['Auteur_number']=df['Auteur'].map({'EAP': 1, 'HPL':2, 'MWS':3})\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "     Auteur  Auteur_number                                              Texte  \\\n0       MWS              3  Idris was well content with this resolve of mi...   \n1       HPL              2  I was faint, even fainter than the hateful mod...   \n2       EAP              1  Above all, I burn to know the incidents of you...   \n3       EAP              1  He might see, perhaps, one or two points with ...   \n4       MWS              3  All obeyed the Lord Protector of dying England...   \n...     ...            ...                                                ...   \n3595    MWS              3  My countenance was painted with the hues of il...   \n3596    HPL              2  It must have been merely the association of an...   \n3597    HPL              2  Dangers he met unflinchingly; crimes he commit...   \n3598    MWS              3  As this state of wretchedness became more conf...   \n3599    MWS              3  Were the pride of ancestry, the patrician spir...   \n\n               File_Name Tokens Tokens_NoStopW lemma PartOfSpeech  ent  \n0     doc_id00001MWS.txt    NaN            NaN   NaN          NaN  NaN  \n1     doc_id00002HPL.txt    NaN            NaN   NaN          NaN  NaN  \n2     doc_id00003EAP.txt    NaN            NaN   NaN          NaN  NaN  \n3     doc_id00004EAP.txt    NaN            NaN   NaN          NaN  NaN  \n4     doc_id00005MWS.txt    NaN            NaN   NaN          NaN  NaN  \n...                  ...    ...            ...   ...          ...  ...  \n3595  doc_id05087MWS.txt    NaN            NaN   NaN          NaN  NaN  \n3596  doc_id05088HPL.txt    NaN            NaN   NaN          NaN  NaN  \n3597  doc_id05089HPL.txt    NaN            NaN   NaN          NaN  NaN  \n3598  doc_id05090MWS.txt    NaN            NaN   NaN          NaN  NaN  \n3599  doc_id05092MWS.txt    NaN            NaN   NaN          NaN  NaN  \n\n[3600 rows x 9 columns]\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "to_Dataframe(chemin)\n",
    "print(df)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-4-fe8fec62be56>\"\u001b[1;36m, line \u001b[1;32m31\u001b[0m\n\u001b[1;33m    '''Nettoyage de la colonne lemma de la lemmatization'''\u001b[0m\n\u001b[1;37m                                        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ],
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-4-fe8fec62be56>, line 31)",
     "output_type": "error"
    }
   ],
   "source": [
    "def tokenization(Texte):\n",
    "    '''Fonction pour tokeniser le texte. Il faut donner comme paramètre la colonne où se trouve le texte.\n",
    "    Ici: df[\"Texte]'''\n",
    "    tokens = []\n",
    "    lemma = []\n",
    "    pos = []\n",
    "    stop_words = []\n",
    "    ent = []\n",
    "    for doc in nlp.pipe(Texte.astype('unicode').values, batch_size=50, n_threads=3):\n",
    "        if doc.is_parsed:\n",
    "            tokens.append([n.text for n in doc])\n",
    "            lemma.append([n.lemma_ for n in doc])\n",
    "            pos.append([n.pos_ for n in doc])\n",
    "            stop_words.append([n.text for n in doc if not n.is_stop])\n",
    "            ent.append([e.label_ for e in doc.ents])\n",
    "        else:\n",
    "            # We want to make sure that the lists of parsed results have the\n",
    "            # same number of entries of the original Dataframe, so add some blanks in case the parse fails\n",
    "            tokens.append(None)\n",
    "            lemma.append(None)\n",
    "            pos.append(None)\n",
    "            stop_words.append(None)\n",
    "            ent.append(None)\n",
    "    df['Tokens'] = tokens\n",
    "    df['Tokens_NoStopW'] = stop_words\n",
    "    df['lemma'] = lemma\n",
    "    df['PartOfSpeech'] = pos\n",
    "    df['ent'] = ent\n",
    "'''\n",
    "def final_clean(df):\n",
    "    '''Nettoyage de la colonne lemma de la lemmatization'''\n",
    "    df['clean_text']=str()\n",
    "    df['clean_text']=''.join(df['lemma'])\n",
    "    df['clean_text']=str(df['clean_text'])\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokenization(df[\"Texte\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "final_clean(df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_finale=df[['Auteur_number','clean_text','ent']]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def WordCloud_texte():\n",
    "    texte_HPL = []\n",
    "    texte_EAP = []\n",
    "    texte_MWS = []\n",
    "    for i, row in df.iterrows():\n",
    "        if row[\"Auteur\"] == 'EAP':\n",
    "            texte_EAP.append(row[\"Tokens_NoStopW\"])\n",
    "        if row[\"Auteur\"] == 'HPL':\n",
    "            texte_HPL.append(row[\"Tokens_NoStopW\"])\n",
    "        if row[\"Auteur\"] == 'MWS':\n",
    "            texte_MWS.append(row[\"Tokens_NoStopW\"])\n",
    "    texte_EAP = ''.join(str(v) for v in texte_EAP)\n",
    "    texte_HPL = ''.join(str(v) for v in texte_HPL)\n",
    "    texte_MWS = ''.join(str(v) for v in texte_MWS)\n",
    "\n",
    "    wordcloud_texte_EAP = WordCloud().generate(texte_EAP)\n",
    "    plt.imshow(wordcloud_texte_EAP, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "    wordcloud_texte_HPL = WordCloud().generate(texte_HPL)\n",
    "    plt.imshow(wordcloud_texte_HPL, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "    wordcloud_texte_MWS = WordCloud().generate(texte_MWS)\n",
    "    plt.imshow(wordcloud_texte_MWS, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "def WorldCloud_ent():\n",
    "    ENT_HPL = []\n",
    "    ENT_EAP = []\n",
    "    ENT_MWS = []\n",
    "    for i, row in df.iterrows():\n",
    "        if row[\"Auteur\"] == 'EAP':\n",
    "            ENT_EAP.append(row[\"ent\"])\n",
    "        if row[\"Auteur\"] == 'HPL':\n",
    "            ENT_HPL.append(row[\"ent\"])\n",
    "        if row[\"Auteur\"] == 'MWS':\n",
    "            ENT_MWS.append(row[\"ent\"])\n",
    "    ENT_EAP = ''.join(str(v) for v in ENT_EAP)\n",
    "    ENT_HPL = ''.join(str(v) for v in ENT_HPL)\n",
    "    ENT_MWS = ''.join(str(v) for v in ENT_MWS)\n",
    "\n",
    "    wordcloud_ENT_EAP = WordCloud().generate(ENT_EAP)\n",
    "    plt.imshow(wordcloud_ENT_EAP, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "    wordcloud_ENT_HPL = WordCloud().generate(ENT_HPL)\n",
    "    plt.imshow(wordcloud_ENT_HPL, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "    wordcloud_ENT_MWS = WordCloud().generate(ENT_MWS)\n",
    "    plt.imshow(wordcloud_ENT_MWS, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "def clean_token():\n",
    "    df['clean_text'] = str()\n",
    "    for i, row in df.iterrows():\n",
    "        row['clean_text'] = ' '.join(row['lemma'])\n",
    "    df['clean_text'] = df.clean_text.replace(\"[PRON\\s\\W]\", \" \", regex=True)\n",
    "    df['clean_text'] = df.clean_text.replace(' +', ' ', regex=True)\n",
    "    df['clean_text'] = df.clean_text.replace('^ ', '', regex=True)\n",
    "\n",
    "\n",
    "def auteur_map():\n",
    "    df['Auteur_number'] = df['Auteur'].map({'EAP': 1, 'HPL': 2, 'MWS': 3})\n",
    "\n",
    "df_finale=df[['Auteur_number','clean_text','ent']]\n",
    "\n",
    "\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}